{
  "metadata": {
    "anaconda-cloud": {},
    "celltoolbar": "Edit Metadata",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "hide": true
      },
      "source": [
        "# The Held Out Set and Model Selection\n",
        "\n",
        "We can fit data using models in a given hypothesis set, say, ${\n",
        "\\cal H_3}$. This the set of all cubics. But how do we choose the \"correct\" hypothesis set, that is, the hypothesis set which has a model which gives is the least training error or cost?\n",
        "\n",
        "This process is called **model selection**, and for this we need a hold out set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "collapsed": false,
        "hide": true,
        "jupyter": {
          "outputs_hidden": false
        }
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import scipy as sp\n",
        "import matplotlib as mpl\n",
        "import matplotlib.cm as cm\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "collapsed": false,
        "hide": true,
        "jupyter": {
          "outputs_hidden": false
        }
      },
      "outputs": [],
      "source": [
        "def make_simple_plot():\n",
        "    fig, axes=plt.subplots(figsize=(12,5), nrows=1, ncols=2);\n",
        "    axes[0].set_ylabel(\"$y$\")\n",
        "    axes[0].set_xlabel(\"$x$\")\n",
        "    axes[1].set_xlabel(\"$x$\")\n",
        "    axes[1].set_yticklabels([])\n",
        "    axes[0].set_ylim([-2,2])\n",
        "    axes[1].set_ylim([-2,2])\n",
        "    plt.tight_layout();\n",
        "    return axes\n",
        "def make_plot():\n",
        "    fig, axes=plt.subplots(figsize=(20,8), nrows=1, ncols=2);\n",
        "    axes[0].set_ylabel(\"$p_R$\")\n",
        "    axes[0].set_xlabel(\"$x$\")\n",
        "    axes[1].set_xlabel(\"$x$\")\n",
        "    axes[1].set_yticklabels([])\n",
        "    axes[0].set_ylim([0,1])\n",
        "    axes[1].set_ylim([0,1])\n",
        "    axes[0].set_xlim([0,1])\n",
        "    axes[1].set_xlim([0,1])\n",
        "    plt.tight_layout();\n",
        "    return axes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Revisiting the model\n",
        "\n",
        "Let $x$ be the fraction of religious people in a county and $y$ be the probability of voting for Romney as a function of $x$. In other words $y_i$ is data that pollsters have taken which tells us their estimate of people voting for Romney and $x_i$ is the fraction of religious people in county $i$. Because poll samples are finite, there is a margin of error on each data point or county $i$, but we will ignore that for now."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let us assume that we have a \"population\" of 200 counties $x$:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        }
      },
      "outputs": [],
      "source": [
        "dffull=pd.read_csv(\"data/religion.csv\")\n",
        "dffull.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Lets suppose now that the Lord came by and told us that the points in the plot below captures $f(x)$ exactly. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        }
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize = (9,6))\n",
        "x=dffull.rfrac.values\n",
        "f=dffull.promney.values\n",
        "plt.plot(x,f,'.', alpha=0.3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Notice that our sampling of $x$ is not quite uniform: there are more points around $x$ of 0.7.\n",
        "\n",
        "Now, in real life we are only given a sample of points. Lets assume that out of this population of 200 points we are given a sample $\\cal{D}$ of 30 data points. Such data is called **in-sample data**. Contrastingly, the entire population of data points is also called **out-of-sample data**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        }
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"data/noisysample.csv\")\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Testing and Training Sets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The process of learning has two parts:\n",
        "\n",
        "1. Fit for a model by minimizing the in-sample risk\n",
        "2. Hope that the in-sample risk approximates the out-of-sample risk well.\n",
        "\n",
        "Mathematically, we are saying that:\n",
        "\n",
        "$$\n",
        "\\begin{eqnarray*}\n",
        "A &:& R_{\\cal{D}}(g) \\,\\,smallest\\,on\\,\\cal{H}\\\\\n",
        "B &:& R_{out \\,of \\,sample} (g) \\approx R_{\\cal{D}}(g)\n",
        "\\end{eqnarray*}\n",
        "$$\n",
        "\n",
        "Hoping does not befit us as scientists. How can we test that the in-sample risk approximates the out-of-sample risk well?\n",
        "\n",
        "The \"aha\" moment comes when we realize that we can **hold back** some of our sample, and test the performance of our learner by trying it out on this held back part! Perhaps we can compute the error or risk on the held-out part, or \"test\" part of our sample, and have something to say about the out-of-sample error."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let us introduce some new terminology. We take the sample of data $\\cal{D}$ that we have been given (our in-sample set) and split it into two parts:\n",
        "\n",
        "1. The **training set**, which is the part of the data we use to fit a model\n",
        "2. The **testing set**, a smaller part of the data set which we use to see how good our fit was.\n",
        "\n",
        "This split is done by choosing points at random into these two sets. Typically we might take 80% of our data and put it in the training set, with the remaining amount going into the test set. This can be carried out in python using the `train_test_split` function from `sklearn.model_selection`.\n",
        "\n",
        "The split is shown in the diagram below:\n",
        "\n",
        "![m:caption](images/train-test.png)\n",
        "\n",
        "We **ARE taking a hit** on the amount of data we have to train our model. The more data we have, the better we can do for our fits. But, you cannot figure out the generalization ability of a learner by looking at the same data it was trained on: there is nothing to generalize to, and as we know we can fit very complex models to training data which have no hope of generalizing (like an interpolator). Thus, to estimate the **out-of-sample error or risk**, we must leave data over to make this estimation. \n",
        "\n",
        "At this point you are thinking: the test set is just another sample of the population, just like the training set. What guarantee do we have that it approximates the out-of-sample error well? And furthermore, if we pick 6 out of 30 points as a test set, why would you expect the estimate to be any good?\n",
        "\n",
        "It can be shown that the test set error is a good estimate of the out of sample error, especially for larger and larger test sets. You are right to worry that 6 points is perhaps too few, but thats what we have for now, and we shall work with them. This is the subject of complexity theory.\n",
        "\n",
        "We are **using the training set then, as our in-sample set, and the test set as a proxy for out-of-sample.**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        }
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "datasize=df.shape[0]\n",
        "#split dataset using the index, as we have x,f, and y that we want to split.\n",
        "itrain,itest = train_test_split(range(30),train_size=24, test_size=6)\n",
        "xtrain= df.x[itrain].values\n",
        "ftrain = df.f[itrain].values\n",
        "ytrain = df.y[itrain].values\n",
        "xtest= df.x[itest].values\n",
        "ftest = df.f[itest].values\n",
        "ytest = df.y[itest].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        }
      },
      "outputs": [],
      "source": [
        "axes=make_plot()\n",
        "axes[0].plot(df.x,df.f, 'k-', alpha=0.6, label=\"f (from the Lord)\");\n",
        "axes[0].plot(df.x,df.y, 'o',alpha=0.6, label=\"$\\cal{D}$\");\n",
        "axes[1].plot(df.x,df.f, 'k-', alpha=0.6, label=\"f (from the Lord)\");\n",
        "axes[1].plot(xtrain, ytrain, 's', label=\"training\")\n",
        "axes[1].plot(xtest, ytest, 's', label=\"testing\")\n",
        "axes[0].legend(loc=\"lower right\")\n",
        "axes[1].legend(loc=\"lower right\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "def make_features(train_set, test_set, degrees):\n",
        "    traintestlist=[]\n",
        "    for d in degrees:\n",
        "        traintestdict={}\n",
        "        traintestdict['train'] = PolynomialFeatures(d).fit_transform(train_set.reshape(-1,1))\n",
        "        traintestdict['test'] = PolynomialFeatures(d).fit_transform(test_set.reshape(-1,1))\n",
        "        traintestlist.append(traintestdict)\n",
        "    return traintestlist"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## How do training and testing error change with complexity?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You will recall that the big question we were left with earlier is: what order of polynomial should we use to fit the data? Which order is too biased? Which one has too much variance and is too complex? Let us try and answer this question.\n",
        "\n",
        "We do this by fitting many different models (remember the fit is made by minimizing the empirical risk on the training set), each with increasing dimension `d`, and looking at the training-error and the test-error in each of these models. So we first try $\\cal{H}_0$, then $\\cal{H}_1$, then $\\cal{H}_2$, and so on.\n",
        "\n",
        "So, for increasing polynomial degree, and thus feature dimension `d`, we fit a `LinearRegression` model on the traing set. We then use scikit-learn again to calculate the error or risk. We calculate the `mean_squared_error` between the model's predictions and the data, BOTH on the training set and test set. We plot this error as a function of the defree of the polynomial `d`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        }
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "degrees=range(21)\n",
        "error_train=np.empty(len(degrees))\n",
        "error_test=np.empty(len(degrees))\n",
        "\n",
        "traintestlists=make_features(xtrain, xtest, degrees)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        }
      },
      "outputs": [],
      "source": [
        "traintestlists[3]['train'], ytrain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        }
      },
      "outputs": [],
      "source": [
        "traintestlists[3]['test'], ytest"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Estimating the out-of-sample error\n",
        "\n",
        "We can then use `mean_squared_error` from `sklearn` to calculate the error between the predictions and actual `ytest` values. Below we calculate this error on both the training set (which we already fit on) and the test set (which we hadnt seen before), and plot how these errors change with the degree of the polynomial."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        }
      },
      "outputs": [],
      "source": [
        "est3 = LinearRegression(fit_intercept=False)\n",
        "est3.fit(traintestlists[3]['train'], ytrain)\n",
        "pred_on_train3=est3.predict(traintestlists[3]['train'])\n",
        "pred_on_test3=est3.predict(traintestlists[3]['test'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        }
      },
      "outputs": [],
      "source": [
        "print(\"errtrain\",mean_squared_error(ytrain, pred_on_train3))\n",
        "print(\"errtest\",mean_squared_error(ytest, pred_on_test3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let us now do this for a polynomial of degree 19"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        }
      },
      "outputs": [],
      "source": [
        "est19 = LinearRegression(fit_intercept=False)\n",
        "est19.fit(traintestlists[19]['train'], ytrain)\n",
        "pred_on_train19=est19.predict(traintestlists[19]['train'])\n",
        "pred_on_test19=est19.predict(traintestlists[19]['test'])\n",
        "print(\"errtrain\",mean_squared_error(ytrain, pred_on_train19))\n",
        "print(\"errtest\",mean_squared_error(ytest, pred_on_test19))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You can see that the test set error is larger, corresponding to an overfit model thats doing very well on some points and awful on other.\n",
        "\n",
        "\n",
        "## Finding the appropriate complexity\n",
        "\n",
        "Lets now carry out this minimization systematically for each polynomial degree d."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "for d in degrees:#for increasing polynomial degrees 0,1,2...\n",
        "    Xtrain = traintestlists[d]['train']\n",
        "    Xtest = traintestlists[d]['test']\n",
        "    #set up model\n",
        "    #fit\n",
        "    #predict\n",
        "    #calculate mean squared error\n",
        "    #set up model\n",
        "    est = LinearRegression(fit_intercept=False)\n",
        "    #fit\n",
        "    est.fit(Xtrain, ytrain)\n",
        "    #predict\n",
        "    prediction_on_training = est.predict(Xtrain)\n",
        "    prediction_on_test = est.predict(Xtest)\n",
        "    #calculate mean squared error\n",
        "    error_train[d] = mean_squared_error(ytrain, prediction_on_training)\n",
        "    error_test[d] = mean_squared_error(ytest, prediction_on_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        }
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize = (9,6))\n",
        "plt.plot(degrees, error_train, marker='o', label='train (in-sample)')\n",
        "plt.plot(degrees, error_test, marker='o', label='test')\n",
        "plt.axvline(np.argmin(error_test), 0,0.5, color='r', label=\"min test error at d=%d\"%np.argmin(error_test), alpha=0.3)\n",
        "plt.ylabel('mean squared error')\n",
        "plt.xlabel('degree')\n",
        "plt.legend(loc='upper left')\n",
        "plt.yscale(\"log\")\n",
        "plt.grid()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The graph shows a very interesting structure. The training error decreases with increasing degree of the polynomial. This ought to make sense given what you know now: one can construct an arbitrarily complex polynomial to fit all the training data: indeed one could construct an order 24 polynomial which perfectly interpolates the 24 data points in the training set. You also know that this would do very badly on the test set as it would wiggle like mad to capture all the data points. And this is indeed what we see in the test set error. \n",
        "\n",
        "For extremely low degree polynomials like $d=0$ a flat line capturing the mean value of the data or $d=1$ a straight line fitting the data, the polynomial is not curvy enough to capturve the curves of the data. We are in the bias/deterministic error regime, where we will always have some difference between the data and the fit since the hypothesis is too simple. But, for degrees higher than 10 or so, the polynomial starts to wiggle too much to capture the training data. The test set error increases as the predictive power of the polynomial goes down thanks to the contortions it must endure to fit the training data.\n",
        "\n",
        "\n",
        "\n",
        "In a long shallow trough at the bottom almost any polynomial model will do well.\n",
        "\n",
        "Thus the test set error first decreases as the model get more expressive, and then, once we exceed a certain level of complexity (here indexed by $d$), it increases. This idea can be used to identify just the right amount of complexity in the model by picking as **the best hypothesis as the one that minimizes test set error** or risk. In our case this happens around $d=1-4$. (This exact number will depend on the random points chosen into the training and test sets) For complexity lower than this critical value, identified by the red vertical line in the diagram, the hypotheses underfit; for complexity higher, they overfit.\n",
        "\n",
        "![m:caption](images/complexity-error-plot.png)\n",
        "\n",
        "Keep in mind that as you see in the plot above this minimum can be shallow: in this case any of the low order polynomials would be \"good enough\". Think why you might want to go as left as you can."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Is this still a test set?\n",
        "\n",
        "But something should be troubling you about this discussion. We have made no discussion on the error bars on our error estimates, primarily because we have not carried out any resampling to make this possible. \n",
        "\n",
        "But secondly we seem to be \"visually fitting\" a value of $d$. It cant be kosher to use as a test set something you did some fitting on...\n",
        "\n",
        "We have contaminated our test set. The moment we **use it in the learning process, it is not a test set**.\n",
        "\n",
        "The answer to the second question is to use a validation set, and leave a separate test set aside. The answer to the first is to use cross-validation, which is a kind of resampling method that uses multiple validation sets!"
      ]
    }
  ]
}